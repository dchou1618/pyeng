{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1209fb2",
   "metadata": {},
   "source": [
    "## NLP Foundations\n",
    "* Mostly Solved: Spam detection, Named Entity Recognition, POS \n",
    "* Good Progress: Sentiment Analysis, Parsing, Information Extraction\n",
    "* Hard: Q&A, Summarization, Paraphrase\n",
    "\n",
    "**Ambiguity is pervasive:** Phrase Structure Parse - noun/verb phrases. Depending how a sentence is interpreted, we could have different phrase structures. \"Federal raises interest rates.\" Raises could be the verb. Or interest could be the verb.\n",
    "* Difficulty comes from non-standard english, correct segmentation, idioms (dark horse), trick entity names (when movie names start and end).\n",
    "* Use probabilistic models from language data.\n",
    "* Regular Language: disjunction [wW]oodchuck and ranges can involve [A-Za-z] to cover all letters lowercase/uppercase. We can have negation in disjunction. [^Ss] or [^A-Z]. \n",
    "    * Pipe: disjunction meaning \"or\".\n",
    "    * ? matches optional previous character.\n",
    "    * * is a kleene star or matches 0 or more of a previous character.\n",
    "    * + matches to 1 or more previous character.\n",
    "    * . matches with anything (only 1 character).\n",
    "    * Anchor: ^[A-Z] is for the start (capital letters at start).\n",
    "        * \\\\.\\$ means string should end with a period.\n",
    "    * Processing of modifying regexes involves minimizing false positives or negatives. We add more to regex to qualify it.\n",
    "* Lexer - takes sequences and cuts it to pieces of tokens (tokenizer).\n",
    "* Tokenization - found that vocabulary increases more than O($\\sqrt{N}$) for N being the number of tokens.\n",
    "    * Type - unique words in vocabulary.\n",
    "    * Tokenization can have issues regarding apostrophes - removes those and results in standalone letters such as \"d\".\n",
    "    * To achieve it on text, you can use piping: $\\verb|tr -sc \"A-Za-z\" \"\\n\" | sort | uniq -c | sort -r -n | less |$. c is the complement so we replace all characters that aren't alphabetical with a newline.\n",
    "    * Tokenization issues with other languages is the absence of spaces in chinese/japanese. L'ensemble should be broken up as two tokens or remain as one token? These are some challenges.\n",
    "    * To tackle this, we apply a baseline tokenization algorithm: maximum-matching. \n",
    "        * (1) Begin with a wordlist in chinese and a string. Direct a pointer to start of string and find longest string matching starting at pointer. Move the pointer over and repeat. Thetabledownthere - probabilistic segmentation algorithm.\n",
    "* Normalize/Stem Words - index/query terms should be matched. For information retrieval, we need to ensure U.S.A and USA are mapped to the same.\n",
    "    * Alternative asymmetric expansion: Enter: window, Search: windows.\n",
    "    * Case folding - generally reduce all letters to lowercase. But may not if capital in middle of sentence. SAIL vs sail or Fed vs. fed.\n",
    "    * Lemmatization - reduce variants to the base form. am, are, is -> be. This is significant in machine translation - quiero (I want) and quieres (You want) have the same lemma of querer.\n",
    "    * Stemming - crude chopping off of affixes. Stemming is simplified version of lemmatization.\n",
    "    * Topic of looking at parts of words: Morpheme (meaningful unit of language that cannot be further divided) - stems (meaning bearing) and affixes (pieces that adhere to stems - grammatical functions)\n",
    "    * Porter's Algorithm (replace rules) - sses -> ss (chops off es). Rules are implemented in order. (*vowel*)ing -> none, (*vowel*)ed -> none. For longer stems, al -> none. Corpus linguistics in Linux.\n",
    "    * Sentence Segmentation - ., ? are ambiguous since . can end an abbreviation or numbers like 4.3. One can use a decision tree to determine whether . is the end of a sentence or not.\n",
    "        * Features in decision tree classifier can be more advanced - case of word after \".\" - probability of word with \".\" occurring at end of sentence.\n",
    "* Similarity between 2 strings: use embedding and use cosine similarity. Or minimum edit distance - editing done through substitution, deletion, insertion. Two words & alignment.\n",
    "    * In **levenstein distance**, substitution has double cost of deletion and insertion.\n",
    "    * In machine translation, we want to see how well translation perform.\n",
    "    * For named entity extraction and entity coreference, if IBM and IBM Inc. are the same. Number of words different to improve entity identification. \n",
    "    * Define initial state, operators, goal state, and path cost - cannot search whole space (shortest path to each revisited state).\n",
    "    * Edit distance can be done using dynamic programming - combining solutions to subproblems.\n",
    "    * Minimum edit distance may not be enough - we need to align the characters of two strings to each other. To do this, we keep a backtrace. The backtrace would include multiple pointers to the possible entries that could have lead to a specific cell. This reveals a path starting from the last indices to [0][0] \n",
    "    * **Weighted Edit Distance** - spell correction (some letters more likely than others). a is more likely to be confused with e,i,o. Modified algorithm to add cost of each deletion/insertion/substitution costs.\n",
    "    * **Minimum Edit Distance in Computational Biology** - maximizes similarity/score instead of minimizing weight in NLP - uses needle-wunsch algorithm - d being deletion/insertion while substitution has custom costs. Origin at upper left for comp bio. Initialization is at -i*d, -j*d, where d is the cost of deleting/inserting something. When updating deletion/insertion, you subtract d. For substitution, you add substitution cost.\n",
    "        * To accomodate for number of gaps at start/end, we don't penalize gaps. **Overlap detection variant** - initialization so we don't have costs to delete or insert everything. We initialize costs where either index is 0 to be 0, where we terminate from the max of the maxes from last column/row. \n",
    "        * **Smith-Waterman Algorithm (Local alignment problem)** - we would want to optimize alignment - similar maximized. So we ignore badly aligned regions. For iteration, we consider maximums of 0, deletion/insertion reduced by d, and additional substitution cost. We trace back in the matrix to identify the optimal alignments leading to maximized similarity - +1 for match, -1 for del/ins/sub.\n",
    "* Measure how good a translation algorithm is by probability of sequence of words. Probabilistic language models are used.\n",
    "    * Joint probability of words in a sentence: \n",
    "    * Count and divide all such sentence, but just too many. The markov assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d9822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f11421e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_distance(s1,s2):\n",
    "    table = [[0]*len(s1) for i in range(len(s2))]\n",
    "    \n",
    "def weighted_edit_distance(s1,s2):\n",
    "    '''\n",
    "    :param: s1 - string\n",
    "    :param: s2 - string\n",
    "    :return: minimum cost edit distance when weighing substitutions based on . \n",
    "    '''\n",
    "    pass\n",
    "    \n",
    "def overlap_detection(seq1,seq2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e7cb063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smith-Waterman Algorithm\n",
    "\n",
    "def smith_waterman():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2fa5de",
   "metadata": {},
   "source": [
    "## NLP Understanding\n",
    "* Task of Relation Extraction - extraction of triplets (relation_name, first_item, second_item)\n",
    "    * The first item is related to the second in some way.\n",
    "    * Ex: (founders, SpaceX, Elon_Musk)\n",
    "    * Automating this can quickly expand knowledge base.\n",
    "    * 60's/70's - Query \"X is the founder of Y\". Consider variants of the query, but may not consider \"Elon Musk, co-founder of SpaceX...\"\n",
    "    * Statistics/Data - feature representation to allow for better generalization. Constraint: **Manually annotate** each sentence. Negative examples would not have a relation.\n",
    "        * **Innovation** - distant supervision. Just use external resource as source of truth. Search corpus and identify sentence with two items, assuming they have the relation specified in knowledge base. 100 times as much data.\n",
    "            * **Limitations** - makes assumption of co-occurrence. Injects noise to dataset (noisy labels). Only able to extend knowledge, but cannot be used to create new knowledge base.\n",
    "    * Polysemy/Synonymy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21559047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e6bd00f",
   "metadata": {},
   "source": [
    "## Information Retrieval\n",
    "* Boolean Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade8ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
